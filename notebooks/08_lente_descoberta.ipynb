{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a174ca-3c33-43ac-a0ef-b62ce0e24fd2",
   "metadata": {},
   "source": [
    "# 8 · A Ótica Adequada: Por Que a Escala Logarítmica É Estrutural\n",
    "\n",
    "**Registro observacional associado ao livro**  \n",
    "*Descobrindo o Caos nos Números Primos — Investigações Computacionais sob o Espelho de Euler*  \n",
    "© Alvaro Costa, 2025  \n",
    "\n",
    "Este notebook faz parte de uma sequência canônica de registros computacionais. Ele não introduz hipóteses, conjecturas ou modelos interpretativos novos.\n",
    "\n",
    "Seu objetivo é exclusivamente **registrar** o comportamento de estruturas aritméticas sob um regime de observação explícito, determinístico e reproduzível.\n",
    "\n",
    "A leitura conceitual completa encontra-se no livro. Este notebook documenta apenas o experimento correspondente.\n",
    "\n",
    "**Licença:** Creative Commons BY–NC–ND 4.0  \n",
    "É permitida a leitura, execução e citação. Não é permitida a modificação, redistribuição adaptada ou uso comercial independente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606782c-3cd5-4ec6-a1d6-70e690b7e33c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. O Mistério das Duas Músicas\n",
    "\n",
    "Nos capítulos anteriores, celebramos a emergência da \"música\" da GOE a partir do nosso operador $ M $. No entanto, uma análise mais atenta revela um mistério: dependendo de como olhamos para a reta numérica, a mesma partitura soa diferente.\n",
    "\n",
    "Quando usamos uma amostragem **linear**, o espectro de $ M $ se aproxima de uma estatística **Poisson**. Já sob uma amostragem **logarítmica**, surge claramente a **GOE**. Por que a mesma metodologia, aplicada à mesma região, produz duas \"músicas\" tão distintas? Seria um artefato?\n",
    "\n",
    "A resposta é não. Estamos diante de um fenômeno fundamental: a forma como olhamos o sistema altera o que o sistema revela. A chave está na escala natural dos próprios números primos.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. As Duas Lentes e a Escala Natural dos Primos\n",
    "\n",
    "A escala que naturalmente se ajusta aos primos é a **logarítmica**, como Gauss observou ao notar que a densidade de primos em torno de um número $ x $ é aproximadamente $ 1/\\log(x) $. Para enxergar a estrutura global dos primos, precisamos de uma régua que se expande com eles — e essa régua é a escala logarítmica.\n",
    "\n",
    "Os métodos de amostragem, portanto, funcionam como **duas lentes**: uma \"calibrada\" para essa escala e outra que a ignora.\n",
    "\n",
    "---\n",
    "\n",
    "### A Lente \"Panorâmica\" (Amostragem Logarítmica)\n",
    "\n",
    "Esta é a lente natural, sintonizada com o ritmo dos primos. Ao amostrar pontos em intervalos logarítmicos, olhamos o universo numérico com a régua certa — uma régua que cresce junto com o próprio espaço.\n",
    "\n",
    "- **O que a lente vê:** Nessa visão panorâmica, a função aritmética $ \\Delta_\\pi(x) $ revela suas flutuações harmônicas e ruídos estruturais.  \n",
    "- **A consequência:** A matriz $ M $ resultante é de alta complexidade, semelhante a uma matriz aleatória real simétrica, e o seu espectro exibe a repulsão de níveis característica da **GOE (Gaussian Orthogonal Ensemble)**.\n",
    "\n",
    "---\n",
    "\n",
    "### A Lente \"Microscópio\" (Amostragem Linear)\n",
    "\n",
    "Esta lente aplica uma régua rígida, sem se ajustar à escala logarítmica. É como observar um relevo de montanhas com uma lupa: tudo parece plano, porque a lente vê apenas um fragmento do mundo.\n",
    "\n",
    "- **O que a lente vê:** Dentro de uma região tão estreita, a tendência global desaparece e \\( \\Delta_\\pi(x) \\) parece estável e suave. As flutuações locais tornam-se quase independentes — um regime de **descorrelação**.  \n",
    "- **A consequência:** A matriz $ M $ construída com essa lente é de baixa variabilidade, e o seu espectro se comporta como um sistema de eventos independentes — uma estatística de **Poisson**.\n",
    "\n",
    "---\n",
    "\n",
    "### Tabela Comparativa\n",
    "\n",
    "| Característica | Amostragem Linear (Microscópio) | Amostragem Logarítmica (Panorâmica) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Alinhamento** | Ignora a escala natural dos primos | Alinhada com a escala natural (Gauss) |\n",
    "| **Visão da Função** | Região localmente \"plana\" | Flutuações globais e ruído harmônico |\n",
    "| **Complexidade da Matriz** | Baixa, ordenada, previsível | Alta, complexa, pseudo-aleatória |\n",
    "| **Resultado Espectral** | **Poisson** | **GOE (Gaussian Orthogonal Ensemble)** |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Laboratório da Escala: A Conexão com o Código\n",
    "\n",
    "A célula de código abaixo demonstra o efeito de forma direta. Um detalhe técnico é fundamental para revelar a estrutura de Poisson: a função `local_normalize_spacings`.\n",
    "\n",
    "Como a lente linear enxerga apenas uma região \"quase plana\", a densidade dos autovalores pode variar ligeiramente. A normalização local age como um **ajuste de foco fino** — recalibra cada espaçamento de acordo com o seu entorno imediato, restaurando a visão adequada do regime Poisson.\n",
    "\n",
    "Esse procedimento não cria o padrão; ele o revela. Ao aplicar `local_normalize_spacings`, observamos que a distribuição Poisson aparece com nitidez — provando que ela é uma **característica real do sistema nesta escala**, e não um artefato numérico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ffb1ff-3612-4673-87ce-34aa35804781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaa2f3471b8418ba6b83dd02a4f2085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='N:', index=2, options=(512, 1024, 2048), value=2048), IntSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Requisitos: pandas, matplotlib, numpy, ipywidgets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import time\n",
    "\n",
    "# --- Funções de Geração de Dados e Matriz ---\n",
    "def generate_pi_data(n: int) -> np.ndarray:\n",
    "    \"\"\"Gera um array com todos os primos até n usando um crivo otimizado.\"\"\"\n",
    "    if n < 2: return np.array([], dtype=np.int64)\n",
    "    size = (n - 1) // 2; sieve = np.ones(size, dtype=bool)\n",
    "    limit = int(np.sqrt(n)) // 2\n",
    "    for i in range(limit):\n",
    "        if sieve[i]:\n",
    "            p = 2 * i + 3; start = (p*p - 3) // 2\n",
    "            sieve[start::p] = False\n",
    "    indices = np.where(sieve)[0]; odd_primes = 2 * indices + 3\n",
    "    return np.concatenate((np.array([2], dtype=np.int64), odd_primes))\n",
    "\n",
    "def get_delta_pi_for_points(x_points, primes):\n",
    "    \"\"\"Calcula Δπ(x) para um array de pontos x usando uma lista de primos pré-calculada.\"\"\"\n",
    "    x_int = np.floor(x_points).astype(int)\n",
    "    pi_x = np.searchsorted(primes, x_int, side='right')\n",
    "    pi_x_div_2 = np.searchsorted(primes, x_int // 2, side='right')\n",
    "    return pi_x - 2 * pi_x_div_2\n",
    "    \n",
    "def generate_cos_matrix(fx_values, x_values):\n",
    "    \"\"\"Gera a matriz M a partir dos vetores F(x) e x.\"\"\"\n",
    "    fx = fx_values.astype(np.float64); x = x_values.astype(np.float64)\n",
    "    x[x <= 0] = 1e-12; logx = np.log(x)\n",
    "    C = np.cos(np.outer(fx, logx)); M = C + C.T\n",
    "    std_dev = M.std()\n",
    "    if std_dev > 0:\n",
    "        M -= M.mean()\n",
    "        M /= std_dev\n",
    "    return 0.5 * (M + M.T)\n",
    "\n",
    "# bulk fixo em 90% central (alpha = 0.10)\n",
    "# janela local fixa para unfolding (não otimizada) w = 21\n",
    "def local_normalize_spacings(lam, alpha=0.10, w=21):\n",
    "    \"\"\"\n",
    "    Normaliza os espaçamentos pela sua média local (unfolding).\n",
    "    Esta é a chave para visualizar corretamente a estatística de Poisson.\n",
    "    \"\"\"\n",
    "    N = lam.size\n",
    "    # Pega o \"bulk\" (meio) do espectro para evitar efeitos de borda\n",
    "    k0, k1 = int(alpha * N), int((1 - alpha) * N)\n",
    "    lam_bulk = np.sort(lam)[k0:k1]\n",
    "    \n",
    "    s = np.diff(lam_bulk)\n",
    "    s = s[s > 0]\n",
    "    \n",
    "    if len(s) < w: return s / s.mean() if s.mean() > 0 else s\n",
    "\n",
    "    # Usa uma média móvel para encontrar a densidade local de estados\n",
    "    w = int(w)\n",
    "    if w % 2 == 0: w += 1 # A janela deve ser ímpar\n",
    "    pad = w // 2\n",
    "    s_padded = np.pad(s, (pad, pad), mode='reflect')\n",
    "    local_mean = np.convolve(s_padded, np.ones(w)/w, mode='valid')\n",
    "    \n",
    "    # Evita divisão por zero\n",
    "    local_mean[local_mean == 0] = 1.0\n",
    "    \n",
    "    return s / local_mean\n",
    "\n",
    "# --- A Função Interativa Principal ---\n",
    "def scale_comparison_lab(N=2048, log_X0=8, span=2.4):\n",
    "    \n",
    "    X0 = int(10**log_X0)\n",
    "    \n",
    "    # --- Preparação dos Dados ---\n",
    "    max_x_log = int(np.ceil(X0 * np.exp(span/2)))\n",
    "    max_x_linear = X0 + N\n",
    "    max_x_needed = max(max_x_log, max_x_linear)\n",
    "    pi_x_full = generate_pi_data(max_x_needed)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True) \n",
    "    \n",
    "    # --- Gráfico da Esquerda: Amostragem Linear (Poisson) ---\n",
    "    print(\"\\n--- Processando Escala Linear ---\")\n",
    "    x_linear = np.arange(X0, X0 + N)\n",
    "    fx_linear = get_delta_pi_for_points(x_linear, pi_x_full)\n",
    "    \n",
    "    M_linear = generate_cos_matrix(fx_linear, x_linear)\n",
    "    lam_linear, _ = np.linalg.eigh(M_linear)\n",
    "    # USA A NORMALIZAÇÃO LOCAL PARA OBTER POISSON\n",
    "    s_unfolded_linear = local_normalize_spacings(lam_linear)\n",
    "\n",
    "    # --- Gráfico da Direita: Amostragem Logarítmica (GOE) ---\n",
    "    print(\"\\n--- Processando Escala Logarítmica ---\")\n",
    "    x_log = np.exp(np.linspace(np.log(X0) - span/2, np.log(X0) + span/2, N))\n",
    "    fx_log = get_delta_pi_for_points(x_log, pi_x_full)\n",
    "\n",
    "    M_log = generate_cos_matrix(fx_log, x_log)\n",
    "    lam_log, _ = np.linalg.eigh(M_log)\n",
    "    # Para GOE, a normalização pela média global já funciona bem\n",
    "    s_log = np.diff(np.sort(lam_log)); s_log = s_log[s_log > 0]\n",
    "    s_unfolded_log = s_log / s_log.mean()\n",
    "\n",
    "    # --- Plots Comparativos ---\n",
    "    s_grid = np.linspace(0, 4, 200)\n",
    "    pdf_goe = (np.pi * s_grid / 2) * np.exp(-np.pi * s_grid**2 / 4)\n",
    "    pdf_poisson = np.exp(-s_grid)\n",
    "    \n",
    "    # Plot da Esquerda\n",
    "    ax = axes[0]\n",
    "    ax.hist(s_unfolded_linear, bins='auto', density=True, alpha=0.75, label='Dados (Linear)')\n",
    "    ax.plot(s_grid, pdf_goe, 'r--', lw=2, label='Teoria GOE')\n",
    "    ax.plot(s_grid, pdf_poisson, 'g:', lw=3, label='Teoria Poisson')\n",
    "    ax.set_title(f'a) Escala Linear → Regime Não Correlacionado', fontsize=14)\n",
    "    ax.set_xlabel('s (Espaçamento Normalizado Localmente)'); ax.set_ylabel('Densidade')\n",
    "    ax.set_xlim(0, 4); ax.legend(loc='upper right')\n",
    "    \n",
    "    # Plot da Direita\n",
    "    ax = axes[1]\n",
    "    ax.hist(s_unfolded_log, bins='auto', density=True, alpha=0.75, label='Dados (Log)')\n",
    "    ax.plot(s_grid, pdf_goe, 'r--', lw=2, label='Teoria GOE')\n",
    "    ax.plot(s_grid, pdf_poisson, 'g:', lw=3, label='Teoria Poisson')\n",
    "    ax.set_title(f'b) Escala Logarítmica → Regime Correlacionado', fontsize=14)\n",
    "    ax.set_xlabel('s (Espaçamento Normalizado Globalmente)'); ax.legend(loc='upper right')\n",
    "    ax.set_xlim(0, 4)\n",
    "    \n",
    "    fig.suptitle(f\"Comparação Visual do Efeito da Escala em X₀ = {X0:g}\", fontsize=18, weight='bold')\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# --- Widget Interativo ---\n",
    "interact(scale_comparison_lab, \n",
    "         N=widgets.Dropdown(options=[512, 1024, 2048], value=2048, description='N:'),\n",
    "         log_X0=widgets.IntSlider(min=5, max=8, step=1, value=8, description='X₀=10^', continuous_update=False),\n",
    "         span=widgets.FloatSlider(min=1.0, max=4.0, step=0.1, value=2.4, description='Span (Log):')\n",
    "        );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0b824-0d91-4315-9831-eb9816d89dc6",
   "metadata": {},
   "source": [
    "> Com a prova experimental em mãos de que as duas lentes produzem músicas diferentes, estamos prontos para aprofundar. No próximo capítulo, vamos analisar a \"ótica\" da nossa lente logarítmica, mergulhando na matemática que explica por que ela não apenas funciona, mas é a única lente que poderia funcionar.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
